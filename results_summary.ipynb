{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary - Code Comment Classification\n",
    "\n",
    "This notebook summarizes the complete pipeline results and provides conclusions.\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "1. **Data Cleaning** (`data_cleaning.ipynb`)\n",
    "   - BERT similarity analysis identified most confusable categories\n",
    "   - Automatically merged most \"similar\" category based on cosine similarity.\n",
    "   - Reduced the number of target classes to 4 reducing the chance of misidentification and making all the categories more distinct.\n",
    "   - Split into train and test sets\n",
    "\n",
    "2. **Feature Encoding** (`encoding.ipynb`)\n",
    "   - BERT embeddings: 384 features\n",
    "   - Class one-hot encoding: 306 features\n",
    "   - Metadata features: 5 features\n",
    "   - **Total: 695 features**\n",
    "\n",
    "3. **Baseline Model** (`model_training.ipynb`)\n",
    "   - GridSearchCV with Logistic Regression\n",
    "   - Best CV F1-Macro: 0.832\n",
    "   - Test Accuracy: 83.2%\n",
    "\n",
    "4. **Multi-Model Comparison** (`multi_model_training.ipynb`)\n",
    "   - Tested 4 algorithms\n",
    "   - Logistic Regression: 0.832 (best)\n",
    "   - Linear SVC: 0.821\n",
    "   - SGD Classifier: 0.813\n",
    "   - Random Forest: 0.794 (worst)"
   ],
   "id": "9ab5818f9ec0ade6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusions and Key Findings\n",
    "\n",
    "### Final Model Performance\n",
    "\n",
    "This notebook implements an **automatic category merging** approach based on BERT similarity analysis combined with BERT embeddings and metadata features for classification.\n",
    "\n",
    "**Best Model:** Logistic Regression with RandomOverSampler\n",
    "- **Test Accuracy: 82.2%**\n",
    "- **F1-Macro Score: 0.83**\n",
    "- **Cross-Validation F1-Macro: 0.832**\n",
    "\n",
    "### Per-Category Performance\n",
    "\n",
    "**Strong Categories:**\n",
    "- **DevelopmentNotes (Class 0):** F1=0.91 ✓ - Clear vocabulary, rich in context clues (parameter-related keywords)\n",
    "- **Expand (Class 1):** F1=0.82 ✓ - Largest class after merge, well-defined\n",
    "\n",
    "Overall all classes have the same average F1, recall and precision scores which means that the model can predict with a high confidence rate each category.\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "Tested 4 different algorithms (all with RandomOverSampler):\n",
    "\n",
    "| Model | Mean F1-Macro | Std Dev | Winner |\n",
    "|-------|-------------|---------|--------|\n",
    "| Logistic Regression | 0.832374 | 0.003923 | ✓ Best |\n",
    "| Linear SVC | 0.821078 | 0.007529 | |\n",
    "| SGD Classifier | 0.813973 | 0.019767 | |\n",
    "| Random Forest | 0.794452 | 0.005036 | ❌ Worst |\n",
    "\n",
    "Linear models (Logistic Regression, SVC) significantly outperform Random Forest when using dense BERT embeddings.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This pipeline successfully demonstrates:\n",
    "- **Automatic category merging** based on semantic similarity analysis\n",
    "- **BERT embeddings + metadata** achieve 83.2% accuracy (F1-Macro: 0.83)\n",
    "- **Logistic Regression** outperforms complex models on this task\n",
    "- **Proper data handling** prevents leakage and ensures fair evaluation"
   ],
   "id": "5bba62107a6a3add"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Files\n",
    "\n",
    "### Data Files:\n",
    "- `code-comment-classification-cleaned.csv` - Cleaned dataset (2,812 rows)\n",
    "- `code-comment-classification-train-unbalanced.csv` - Training set (2,249 rows)\n",
    "- `code-comment-classification-test.csv` - Test set (563 rows)\n",
    "\n",
    "### Encoded Features:\n",
    "- `train_features_4cat_bert_meta.npz` - Training features (2,249 × 695)\n",
    "- `test_features_4cat_bert_meta.npz` - Test features (563 × 695)\n",
    "- `train_target_4cat_meta.csv` - Training labels\n",
    "- `test_target_4cat_meta.csv` - Test labels\n",
    "\n",
    "### Encoders:\n",
    "- `class_encoder_4cat_meta.pkl` - OneHotEncoder for class names\n",
    "- `bert_model_4cat_meta.pkl` - SentenceTransformer model\n",
    "- `label_encoder_4cat_meta.pkl` - LabelEncoder for categories\n",
    "\n",
    "### Trained Models:\n",
    "- `best_model_final.pkl` - Best performing model (Logistic Regression)\n",
    "\n",
    "## How to Use\n",
    "\n",
    "### Run the complete pipeline\n",
    "```bash\n",
    "jupyter notebook complete-pipeline.ipynb\n",
    "```\n",
    "### Run each step of the pipeline separately:\n",
    "```bash\n",
    "# 1. Clean data and merge categories\n",
    "jupyter notebook data_cleaning.ipynb\n",
    "\n",
    "# 2. Encode features\n",
    "jupyter notebook encoding.ipynb\n",
    "\n",
    "# 3. Train baseline model\n",
    "jupyter notebook model_training.ipynb\n",
    "\n",
    "# 4. Compare multiple models\n",
    "jupyter notebook multi_model_training.ipynb\n",
    "\n",
    "# 5. View results summary\n",
    "jupyter notebook results_summary.ipynb\n",
    "```\n",
    "\n",
    "### Load and use the trained model:\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model\n",
    "model = joblib.load(\"outputs/best_model_final.pkl\")\n",
    "\n",
    "# Load encoders\n",
    "class_encoder = joblib.load(\"outputs/class_encoder_4cat_meta.pkl\")\n",
    "bert_model = joblib.load(\"outputs/bert_model_4cat_meta.pkl\")\n",
    "label_encoder = joblib.load(\"outputs/label_encoder_4cat_meta.pkl\")\n",
    "\n",
    "# Make predictions on new data\n",
    "# (after encoding features the same way)\n",
    "predictions = model.predict(X_new)\n",
    "```"
   ],
   "id": "af2c598846d1ec4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
