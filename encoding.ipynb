{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Encoding - Code Comment Classification\n",
    "\n",
    "This notebook encodes features using BERT embeddings + metadata + class one-hot encoding.\n",
    "\n",
    "## Steps:\n",
    "1. Load train/test splits from data cleaning\n",
    "2. Extract metadata features\n",
    "3. Encode target labels\n",
    "4. Generate BERT embeddings\n",
    "5. One-hot encode class names\n",
    "6. Combine all features\n",
    "7. Save encoded features\n",
    "\n",
    "## Input Files:\n",
    "- `code-comment-classification-train-unbalanced.csv`\n",
    "- `code-comment-classification-test.csv`\n",
    "\n",
    "## Output Files:\n",
    "- `train_features_4cat_bert_meta.npz`\n",
    "- `test_features_4cat_bert_meta.npz`\n",
    "- `train_target_4cat_meta.csv`\n",
    "- `test_target_4cat_meta.csv`\n",
    "- `class_encoder_4cat_meta.pkl`\n",
    "- `bert_model_4cat_meta.pkl`\n",
    "- `label_encoder_4cat_meta.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# BERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Utilities\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Encoding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load the Split Datasets\n",
    "We load the separate Training and Testing files created in the previous cleaning step. This ensures our test set remains unseen during the fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (2249, 4)\n",
      "Test Set Shape:     (563, 4)\n",
      "Columns: ['comment_sentence_id', 'class', 'comment_sentence', 'category']\n"
     ]
    }
   ],
   "source": [
    "# Load the training data (Use this to learn patterns/vocabulary)\n",
    "df_train = pd.read_csv(\"code-comment-classification-train-unbalanced.csv\")\n",
    "\n",
    "# Load the test data (Use this ONLY for evaluation)\n",
    "df_test = pd.read_csv(\"code-comment-classification-test.csv\")\n",
    "\n",
    "print(f\"Training Set Shape: {df_train.shape}\")\n",
    "print(f\"Test Set Shape:     {df_test.shape}\")\n",
    "\n",
    "# Verify columns\n",
    "print(\"Columns:\", df_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Separate Features and Target\n",
    "We separate the input features (`class`, `comment_sentence`) from the target variable (`category`) for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and Target separated.\n"
     ]
    }
   ],
   "source": [
    "FEATURES = [\"class\", \"comment_sentence\"]\n",
    "TARGET = \"category\"\n",
    "\n",
    "# Split Training Data\n",
    "X_train_enc = df_train[FEATURES].copy()\n",
    "y_train_enc = df_train[TARGET]\n",
    "\n",
    "# Split Test Data\n",
    "X_test_enc = df_test[FEATURES].copy()\n",
    "y_test_enc = df_test[TARGET]\n",
    "\n",
    "print(\"Features and Target separated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract Metadata Features\n",
    "\n",
    "Before encoding, we'll extract additional features that might help classification:\n",
    "- **comment_length**: Number of words in the comment\n",
    "- **has_params**: Whether comment mentions parameter-related words\n",
    "- **has_code_symbols**: Whether comment contains code-related symbols\n",
    "- **starts_with_verb**: Whether comment starts with common action verbs\n",
    "- **has_default**: Whether comment mentions default values\n",
    "\n",
    "These metadata features will be combined with BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata features from training data...\n",
      "Extracting metadata features from test data...\n",
      "\n",
      "Metadata features extracted:\n",
      "  - comment_length: 6.9 words (avg)\n",
      "  - has_params: 632 comments (28.1%)\n",
      "  - has_code_symbols: 838 comments (37.3%)\n",
      "  - starts_with_verb: 35 comments (1.6%)\n",
      "  - has_default: 112 comments (5.0%)\n",
      "\n",
      "Training data shape: (2249, 7)\n",
      "Columns: ['class', 'comment_sentence', 'comment_length', 'has_params', 'has_code_symbols', 'starts_with_verb', 'has_default']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmacera\\AppData\\Local\\Temp\\ipykernel_26800\\2087565256.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df['starts_with_verb'] = df['comment_sentence'].str.contains(verb_pattern, case=False, regex=True).astype(int)\n",
      "C:\\Users\\lmacera\\AppData\\Local\\Temp\\ipykernel_26800\\2087565256.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df['starts_with_verb'] = df['comment_sentence'].str.contains(verb_pattern, case=False, regex=True).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Define metadata extraction functions\n",
    "def extract_metadata_features(df):\n",
    "    \"\"\"Extract metadata features from comment text\"\"\"\n",
    "    \n",
    "    # 1. Comment length (number of words)\n",
    "    df['comment_length'] = df['comment_sentence'].str.split().str.len()\n",
    "    \n",
    "    # 2. Has parameter-related keywords\n",
    "    param_keywords = ['param', 'parameter', 'arg', 'argument', 'int', 'str', 'bool', 'float', 'list', 'dict', 'type']\n",
    "    param_pattern = '|'.join(param_keywords)\n",
    "    df['has_params'] = df['comment_sentence'].str.contains(param_pattern, case=False, regex=True).astype(int)\n",
    "    \n",
    "    # 3. Has code symbols\n",
    "    code_symbols = [r'\\(', r'\\)', r'\\[', r'\\]', r'\\{', r'\\}', r'_', r'\\.']\n",
    "    code_pattern = '|'.join(code_symbols)\n",
    "    df['has_code_symbols'] = df['comment_sentence'].str.contains(code_pattern, regex=True).astype(int)\n",
    "    \n",
    "    # 4. Starts with common action verbs\n",
    "    action_verbs = ['returns', 'return', 'creates', 'create', 'provides', 'provide', 'handles', 'handle', \n",
    "                    'implements', 'implement', 'executes', 'execute', 'generates', 'generate',\n",
    "                    'validates', 'validate', 'processes', 'process', 'manages', 'manage']\n",
    "    verb_pattern = '^(' + '|'.join(action_verbs) + ')'\n",
    "    df['starts_with_verb'] = df['comment_sentence'].str.contains(verb_pattern, case=False, regex=True).astype(int)\n",
    "    \n",
    "    # 5. Mentions default values\n",
    "    df['has_default'] = df['comment_sentence'].str.contains('default', case=False).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to training data\n",
    "print(\"Extracting metadata features from training data...\")\n",
    "X_train_enc = extract_metadata_features(X_train_enc)\n",
    "\n",
    "# Apply to test data\n",
    "print(\"Extracting metadata features from test data...\")\n",
    "X_test_enc = extract_metadata_features(X_test_enc)\n",
    "\n",
    "print(\"\\nMetadata features extracted:\")\n",
    "print(f\"  - comment_length: {X_train_enc['comment_length'].describe()['mean']:.1f} words (avg)\")\n",
    "print(f\"  - has_params: {X_train_enc['has_params'].sum()} comments ({X_train_enc['has_params'].sum()/len(X_train_enc)*100:.1f}%)\")\n",
    "print(f\"  - has_code_symbols: {X_train_enc['has_code_symbols'].sum()} comments ({X_train_enc['has_code_symbols'].sum()/len(X_train_enc)*100:.1f}%)\")\n",
    "print(f\"  - starts_with_verb: {X_train_enc['starts_with_verb'].sum()} comments ({X_train_enc['starts_with_verb'].sum()/len(X_train_enc)*100:.1f}%)\")\n",
    "print(f\"  - has_default: {X_train_enc['has_default'].sum()} comments ({X_train_enc['has_default'].sum()/len(X_train_enc)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train_enc.shape}\")\n",
    "print(f\"Columns: {X_train_enc.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Encode the Target Labels\n",
    "We convert the text labels (e.g., \"Usage\", \"Summary\") into numbers (0, 1, 2...).\n",
    "- We `.fit()` the label encoder only on `y_train`\n",
    "- We check if the test set contains any new labels (unlikely in this dataset, but good practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category to Numeric Mapping:\n",
      "  DevelopmentNotes: 0\n",
      "  Expand: 1\n",
      "  Parameters: 2\n",
      "  Summary: 3\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit on Training labels\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_enc)\n",
    "\n",
    "# Transform Test labels (using the same mapping)\n",
    "y_test_encoded = label_encoder.transform(y_test_enc)\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Category to Numeric Mapping:\")\n",
    "for i, category in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {category}: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Build and Fit the Feature Engineering Pipeline with BERT Embeddings + Metadata\n",
    "\n",
    "We combine three types of features:\n",
    "1. `OneHotEncoder`: Converts the `class` column into binary columns.\n",
    "2. `SentenceTransformer (BERT)`: Converts the `comment_sentence` into dense semantic embeddings.\n",
    "3. `Metadata Features`: The 5 extracted features (comment_length, has_params, etc.)\n",
    "\n",
    "**Hypothesis:** Combining BERT's semantic understanding with explicit metadata features should boost performance beyond 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model (all-MiniLM-L6-v2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded.\n",
      "\n",
      "Encoding training comments with BERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341e22228a74424f8f8543dd268879fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT embeddings shape: (2249, 384)\n",
      "\n",
      "Encoding test comments with BERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bafe5dce5c4429b9c984f599daf782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BERT embeddings shape: (563, 384)\n",
      "\n",
      "One-hot encoding 'class' column...\n",
      "Training class encoding shape: (2249, 306)\n",
      "Test class encoding shape: (563, 306)\n",
      "\n",
      "Extracting metadata features...\n",
      "Training metadata shape: (2249, 5)\n",
      "Test metadata shape: (563, 5)\n",
      "\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT model (downloads ~80MB on first run)\n",
    "print(\"Loading BERT model (all-MiniLM-L6-v2)...\")\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"BERT model loaded.\")\n",
    "\n",
    "# Encode training comments with BERT\n",
    "print(\"\\nEncoding training comments with BERT...\")\n",
    "train_comment_embeddings = bert_model.encode(\n",
    "    X_train_enc['comment_sentence'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "print(f\"Training BERT embeddings shape: {train_comment_embeddings.shape}\")\n",
    "\n",
    "# Encode test comments with BERT\n",
    "print(\"\\nEncoding test comments with BERT...\")\n",
    "test_comment_embeddings = bert_model.encode(\n",
    "    X_test_enc['comment_sentence'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "print(f\"Test BERT embeddings shape: {test_comment_embeddings.shape}\")\n",
    "\n",
    "# Create OneHotEncoder for the 'class' column\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "class_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "\n",
    "# Fit and transform 'class' column\n",
    "print(\"\\nOne-hot encoding 'class' column...\")\n",
    "train_class_encoded = class_encoder.fit_transform(X_train_enc[['class']])\n",
    "test_class_encoded = class_encoder.transform(X_test_enc[['class']])\n",
    "\n",
    "print(f\"Training class encoding shape: {train_class_encoded.shape}\")\n",
    "print(f\"Test class encoding shape: {test_class_encoded.shape}\")\n",
    "\n",
    "# Extract metadata features as numpy arrays\n",
    "print(\"\\nExtracting metadata features...\")\n",
    "metadata_cols = ['comment_length', 'has_params', 'has_code_symbols', 'starts_with_verb', 'has_default']\n",
    "train_metadata = X_train_enc[metadata_cols].values\n",
    "test_metadata = X_test_enc[metadata_cols].values\n",
    "\n",
    "print(f\"Training metadata shape: {train_metadata.shape}\")\n",
    "print(f\"Test metadata shape: {test_metadata.shape}\")\n",
    "\n",
    "print(\"\\nFeature engineering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Combine All Features and Save\n",
    "Now we combine BERT embeddings + class encoding + metadata features into final feature matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Features Shape: (2249, 695)\n",
      "Final Test Features Shape: (563, 695)\n",
      "\n",
      "Feature breakdown:\n",
      "  - Class one-hot: 306 features\n",
      "  - BERT embeddings: 384 features\n",
      "  - Metadata: 5 features\n",
      "  - Total: 695 features\n",
      "\n",
      "Files Saved Successfully:\n",
      "- train_features_4cat_bert_meta.npz & train_target_4cat_meta.csv\n",
      "- test_features_4cat_bert_meta.npz  & test_target_4cat_meta.csv\n",
      "- class_encoder_4cat_meta.pkl\n",
      "- bert_model_4cat_meta.pkl\n",
      "- label_encoder_4cat_meta.pkl\n"
     ]
    }
   ],
   "source": [
    "# Convert to sparse matrices\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "train_bert_sparse = csr_matrix(train_comment_embeddings)\n",
    "test_bert_sparse = csr_matrix(test_comment_embeddings)\n",
    "\n",
    "train_metadata_sparse = csr_matrix(train_metadata)\n",
    "test_metadata_sparse = csr_matrix(test_metadata)\n",
    "\n",
    "# Combine: class encoding + BERT embeddings + metadata features\n",
    "X_train_encoded = hstack([train_class_encoded, train_bert_sparse, train_metadata_sparse])\n",
    "X_test_encoded = hstack([test_class_encoded, test_bert_sparse, test_metadata_sparse])\n",
    "\n",
    "print(f\"Final Training Features Shape: {X_train_encoded.shape}\")\n",
    "print(f\"Final Test Features Shape: {X_test_encoded.shape}\")\n",
    "\n",
    "print(\"\\nFeature breakdown:\")\n",
    "print(f\"  - Class one-hot: {train_class_encoded.shape[1]} features\")\n",
    "print(f\"  - BERT embeddings: {train_bert_sparse.shape[1]} features\")\n",
    "print(f\"  - Metadata: {train_metadata_sparse.shape[1]} features\")\n",
    "print(f\"  - Total: {X_train_encoded.shape[1]} features\")\n",
    "\n",
    "# --- SAVING FILES ---\n",
    "\n",
    "# Save Features (Sparse Matrices)\n",
    "sparse.save_npz(\"train_features_4cat_bert_meta.npz\", X_train_encoded)\n",
    "sparse.save_npz(\"test_features_4cat_bert_meta.npz\", X_test_encoded)\n",
    "\n",
    "# Save Targets (CSVs)\n",
    "pd.DataFrame(y_train_encoded, columns=['category']).to_csv(\"train_target_4cat_meta.csv\", index=False)\n",
    "pd.DataFrame(y_test_encoded, columns=['category']).to_csv(\"test_target_4cat_meta.csv\", index=False)\n",
    "\n",
    "# Save the encoders for later use\n",
    "joblib.dump(class_encoder, \"class_encoder_4cat_meta.pkl\")\n",
    "joblib.dump(bert_model, \"bert_model_4cat_meta.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder_4cat_meta.pkl\")\n",
    "\n",
    "print(\"\\nFiles Saved Successfully:\")\n",
    "print(\"- train_features_4cat_bert_meta.npz & train_target_4cat_meta.csv\")\n",
    "print(\"- test_features_4cat_bert_meta.npz  & test_target_4cat_meta.csv\")\n",
    "print(\"- class_encoder_4cat_meta.pkl\")\n",
    "print(\"- bert_model_4cat_meta.pkl\")\n",
    "print(\"- label_encoder_4cat_meta.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
