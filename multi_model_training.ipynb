{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665a4dcf",
   "metadata": {},
   "source": [
    "## Motivation for Using Multiple Models\n",
    "\n",
    "Initially, we tried `Logistic Regression` as a baseline model because it is:\n",
    "\n",
    "- Fast to train\n",
    "- Well-suited for high-dimensional sparse data (like `TF-IDF`)\n",
    "- Interpretable and widely used in text classification\n",
    "\n",
    "However, after evaluating `Logistic Regression`, we observed:\n",
    "\n",
    "- High overall accuracy (`~0.76`), but extremely low `F1` for the minority class (`0.03`)\n",
    "- The model predicted almost all examples as the majority class due to **class imbalance**\n",
    "- This makes it **unsuitable for our task**, where detecting the minority class (`instance_type=1`) is important\n",
    "\n",
    "### Therefore, we try multiple alternative models to improve performance:\n",
    "\n",
    "1. `LinearSVC`\n",
    "   - `Linear Support Vector Machine` is well-known for text classification  \n",
    "   - Handles high-dimensional sparse data efficiently  \n",
    "   - Works well with imbalanced classes using `class_weight='balanced'`\n",
    "\n",
    "2. `SGDClassifier` (`Hinge` / `Log`)\n",
    "   - Implements stochastic gradient descent for linear models  \n",
    "   - Can be faster than traditional linear models on large datasets  \n",
    "   - Supports `class_weight='balanced'` for imbalanced data\n",
    "\n",
    "3. `Decision Forest` (`RandomForest`)\n",
    "   - Ensemble of decision trees capturing non-linear relationships  \n",
    "   - Robust to noise and outliers  \n",
    "   - Handles categorical and numeric features without much preprocessing\n",
    "\n",
    "4. `Gradient Boosting` (`XGBoost` / `LightGBM`) \n",
    "   - Powerful ensemble models that can learn complex patterns  \n",
    "   - Include mechanisms to handle class imbalance (`scale_pos_weight` or `class_weight`)  \n",
    "   - Often outperform linear models on structured and text-derived features\n",
    "\n",
    "5. `Neural Network` (`MLPClassifier`)\n",
    "   - Can capture non-linear interactions between features  \n",
    "   - Flexible architecture for combining text embeddings (`TF-IDF`) with categorical/numeric features  \n",
    "\n",
    "### Key Goals\n",
    "\n",
    "- Improve detection of minority class (class `1`) without sacrificing too much overall accuracy  \n",
    "- Maximize `F1`-macro, which balances performance across both classes  \n",
    "- Compare models to select the best performing pipeline for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea5023",
   "metadata": {},
   "source": [
    "## 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed948657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:48:17.830814Z",
     "start_time": "2025-12-09T16:48:16.289554Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504e6e9",
   "metadata": {},
   "source": [
    "## 1. Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6dc9545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:48:17.857418Z",
     "start_time": "2025-12-09T16:48:17.836819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (2291, 2)\n",
      "Test size: (573, 2)\n"
     ]
    }
   ],
   "source": [
    "df_clean = pd.read_csv(\"code-comment-classification-cleaned-no-outliers.csv\")\n",
    "\n",
    "FEATURES = [\"class\", \"comment_sentence\"]\n",
    "TARGET = \"category\"\n",
    "\n",
    "X = df_clean[FEATURES]\n",
    "y = df_clean[TARGET]\n",
    "\n",
    "# Train/test split (80/20 stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847c2ae",
   "metadata": {},
   "source": [
    "## 2. Define preprocessing pipeline\n",
    "One pipeline for all models:\n",
    "- `OneHotEncoder` for categorical features\n",
    "- `TF-IDF` for text\n",
    "- passthrough numeric feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6608c868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:48:18.024623Z",
     "start_time": "2025-12-09T16:48:18.021284Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"class\"]),\n",
    "        (\"text\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2)), \"comment_sentence\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783828a5",
   "metadata": {},
   "source": [
    "## 3. Define all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "182371ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:48:18.054977Z",
     "start_time": "2025-12-09T16:48:18.047919Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearSVC\": LinearSVC(class_weight=\"balanced\", max_iter=4000, random_state=42),\n",
    "    \"SGDClassifier_hinge\": SGDClassifier(loss=\"hinge\", class_weight=\"balanced\", max_iter=4000, random_state=42),\n",
    "    \"SGDClassifier_log\": SGDClassifier(loss=\"log_loss\", class_weight=\"balanced\", max_iter=4000, random_state=42),\n",
    "    \"DecisionForest\": RandomForestClassifier(\n",
    "        n_estimators=300, \n",
    "        max_depth=None, \n",
    "        min_samples_split=5, \n",
    "        class_weight=\"balanced\", \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        use_label_encoder=False, \n",
    "        eval_metric=\"logloss\", \n",
    "        n_estimators=200, \n",
    "        scale_pos_weight=len(y[y==1])/len(y[y!=1]), \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "        n_estimators=200, \n",
    "        class_weight=\"balanced\", \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"NeuralNetwork\": MLPClassifier(\n",
    "        hidden_layer_sizes=(100,), \n",
    "        activation='relu', \n",
    "        solver='adam', \n",
    "        max_iter=500, \n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3400cb3",
   "metadata": {},
   "source": [
    "## 4. Train and evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "041339e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:51:20.864654Z",
     "start_time": "2025-12-09T16:48:18.057982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING: LinearSVC ===\n",
      "Accuracy: 0.578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.29      0.29        62\n",
      "           1       0.46      0.41      0.43       101\n",
      "           2       0.72      0.75      0.74       159\n",
      "           3       0.55      0.49      0.52        91\n",
      "           4       0.61      0.67      0.64       160\n",
      "\n",
      "    accuracy                           0.58       573\n",
      "   macro avg       0.53      0.52      0.52       573\n",
      "weighted avg       0.57      0.58      0.57       573\n",
      "\n",
      "\n",
      "=== TRAINING: SGDClassifier_hinge ===\n",
      "Accuracy: 0.567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.37      0.35        62\n",
      "           1       0.45      0.42      0.43       101\n",
      "           2       0.71      0.74      0.72       159\n",
      "           3       0.52      0.44      0.48        91\n",
      "           4       0.61      0.64      0.63       160\n",
      "\n",
      "    accuracy                           0.57       573\n",
      "   macro avg       0.52      0.52      0.52       573\n",
      "weighted avg       0.57      0.57      0.57       573\n",
      "\n",
      "\n",
      "=== TRAINING: SGDClassifier_log ===\n",
      "Accuracy: 0.592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.32      0.32        62\n",
      "           1       0.48      0.46      0.47       101\n",
      "           2       0.73      0.78      0.75       159\n",
      "           3       0.56      0.49      0.52        91\n",
      "           4       0.64      0.65      0.64       160\n",
      "\n",
      "    accuracy                           0.59       573\n",
      "   macro avg       0.54      0.54      0.54       573\n",
      "weighted avg       0.59      0.59      0.59       573\n",
      "\n",
      "\n",
      "=== TRAINING: DecisionForest ===\n",
      "Accuracy: 0.551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.26      0.28        62\n",
      "           1       0.38      0.40      0.39       101\n",
      "           2       0.72      0.71      0.72       159\n",
      "           3       0.51      0.58      0.55        91\n",
      "           4       0.61      0.59      0.60       160\n",
      "\n",
      "    accuracy                           0.55       573\n",
      "   macro avg       0.50      0.51      0.50       573\n",
      "weighted avg       0.55      0.55      0.55       573\n",
      "\n",
      "\n",
      "=== TRAINING: XGBoost ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmacera\\Desktop\\CodeCommentClassification\\venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [23:04:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.21      0.24        62\n",
      "           1       0.47      0.33      0.39       101\n",
      "           2       0.70      0.75      0.72       159\n",
      "           3       0.53      0.51      0.52        91\n",
      "           4       0.55      0.68      0.60       160\n",
      "\n",
      "    accuracy                           0.56       573\n",
      "   macro avg       0.50      0.49      0.49       573\n",
      "weighted avg       0.54      0.56      0.54       573\n",
      "\n",
      "\n",
      "=== TRAINING: LightGBM ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 784\n",
      "[LightGBM] [Info] Number of data points in the train set: 2291, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmacera\\Desktop\\CodeCommentClassification\\venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lmacera\\Desktop\\CodeCommentClassification\\venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.34      0.29        62\n",
      "           1       0.46      0.34      0.39       101\n",
      "           2       0.76      0.64      0.70       159\n",
      "           3       0.32      0.64      0.43        91\n",
      "           4       0.60      0.39      0.47       160\n",
      "\n",
      "    accuracy                           0.48       573\n",
      "   macro avg       0.48      0.47      0.46       573\n",
      "weighted avg       0.54      0.48      0.49       573\n",
      "\n",
      "\n",
      "=== TRAINING: NeuralNetwork ===\n",
      "Accuracy: 0.536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.34      0.32        62\n",
      "           1       0.39      0.38      0.38       101\n",
      "           2       0.72      0.71      0.71       159\n",
      "           3       0.51      0.35      0.42        91\n",
      "           4       0.55      0.64      0.60       160\n",
      "\n",
      "    accuracy                           0.54       573\n",
      "   macro avg       0.49      0.48      0.49       573\n",
      "weighted avg       0.54      0.54      0.53       573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n=== TRAINING: {name} ===\")\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    \n",
    "    # Fit model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_class1\": report[\"1\"][\"f1-score\"],\n",
    "        \"f1_class2\": report[\"2\"][\"f1-score\"],\n",
    "        \"f1_class3\": report[\"3\"][\"f1-score\"],\n",
    "        \"f1_class4\": report[\"4\"][\"f1-score\"],\n",
    "        \"f1_macro\": report[\"macro avg\"][\"f1-score\"]\n",
    "    })\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dee61",
   "metadata": {},
   "source": [
    "## 5. Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cfe7797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:51:20.913686Z",
     "start_time": "2025-12-09T16:51:20.905672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL COMPARISON ===\n",
      "                 model  accuracy  f1_class1  f1_class2  f1_class3  f1_class4  \\\n",
      "0    SGDClassifier_log  0.591623   0.469388   0.751515   0.523256   0.643963   \n",
      "1            LinearSVC  0.577661   0.429319   0.738462   0.520231   0.640719   \n",
      "2  SGDClassifier_hinge  0.567190   0.430769   0.722222   0.476190   0.628049   \n",
      "3       DecisionForest  0.551483   0.386473   0.717460   0.546392   0.598726   \n",
      "4              XGBoost  0.556719   0.385965   0.721212   0.519774   0.603352   \n",
      "5        NeuralNetwork  0.535777   0.381910   0.712934   0.415584   0.595376   \n",
      "6             LightGBM  0.483421   0.388571   0.696246   0.428044   0.469697   \n",
      "\n",
      "   f1_macro  \n",
      "0  0.541624  \n",
      "1  0.524283  \n",
      "2  0.521675  \n",
      "3  0.504983  \n",
      "4  0.493333  \n",
      "5  0.485776  \n",
      "6  0.455253  \n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results).sort_values(by=\"f1_macro\", ascending=False)\n",
    "df_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164a2fd",
   "metadata": {},
   "source": [
    "## 6. SAVE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8df95d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:51:23.048760Z",
     "start_time": "2025-12-09T16:51:20.919493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model: SGDClassifier_log -> best_model_all_models.pkl\n"
     ]
    }
   ],
   "source": [
    "best_model_name = df_results.loc[0, \"model\"]\n",
    "best_model_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", models[best_model_name])\n",
    "])\n",
    "\n",
    "# Fit best model on full training set\n",
    "best_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(best_model_pipeline, \"best_model_all_models.pkl\")\n",
    "print(f\"Saved best model: {best_model_name} -> best_model_all_models.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
